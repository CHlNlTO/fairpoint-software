MAIMULTEATINUTULUIUT NON MINI US010089187B1

(12)United States Patent
Pecoraro et al.

(10) PatentNo.: US 10,089,187 B1
(45) Date of Patent: Oct. 2, 2018

(54) SCALABLE CLOUD BACKUP
(71) Applicant: EMC Corporation, Hopkinton , MA
(US)
(72 ) Inventors: Alex Pecoraro, Seattle, WA (US); Nick Kirsch, Seattle, WA (US); Daniel Forrest, University Place, WA (US); Shaun Edwards, Edgewood, WA (US); Ganesh Prabhu , Bellevue, WA (US)
@ (73) Assignee: EMC IP HOLDING COMPANY
LLC , Hopkinton ,MA (US)

@ ( * ) Notice: Subject to any disclaimer, the term ofthis
patent is extended or adjusted under 35
U .S.C . 154 (b ) by 256 days.
(21) Appl.No.: 15/084,208

@ (22 ) Filed : Mar. 29, 2016

(51) Int. CI.

G06F 1730

(2006 .01)

G06F 11/ 14

(2006 .01)

H04L 29/08

(2006 .01)

(52) U.S. CI.
CPC ...... GOOF 11/1451 (2013 .01); H04L 67/1097

(2013.01); G06F 2201/80 (2013.01)

(58) Field of Classification Search
CPC ............. GO6F 17/302; G06F 17/30082; G06F 17 / 30215

USPC .......
See application file for complete search history.

(56)

References Cited

U.S. PATENTDOCUMENTS

8,285,681 B2* 10/2012 Prahlad ... GO6F 17 /302
707/640
8,612,439 B2* 12/2013 Prahlad ................ G06F 17/302
707/640
2010/0332401 Al* 12/2010 Prahlad ........... GO6F 17/30082 705/80
2010/0332456 A1 * 12/2010 Prahlad ............. GO6F 17/30082 707 /664
2010 /0333116 A1* 12/2010 Prahlad GO6F 17/30082 719 /328
2013/0024424 A1* 1/2013 Prahlad ............. GO6F 17/30082
707/640
2015/0012495 A1* 1/2015 Prahlad ............. GO6F 17/30082
707/640
2017/0039218 A1* 2/2017 Prahlad ............. G06F 17 /30082

* cited by examiner

Primary Examiner -- Jean M Corrielus (74) Attorney, Agent, or Firm -- Krishnendu Gupta ; Sean
M . Evans

(57)

ABSTRACT

Implementations are provided for scalable cloud backup . A

coordinator process can manage worker processes on nodes

to package file system data that is targeted for cloud backup

into node local upload objects . File data can be arranged into

distinct block offsets ofthe node local upload object. A set

ofmetadata tables can be generated that characterize each

file that is backed up as well as file block location informa

tion for each data block . The node local upload objects can

be uploaded to a cloud service provider. The set ofmetadata

tables generated by the worker process can be coalesced into

a global set ofmetadata tables that describe the data that has

been backed up. In one implementation, after an initial cloud

backup has occurred , a snapshot service of the file system

can be used to incrementally backup blocks of the file that

have been changed .

18 Claims, 11 Drawing Sheets

NODE I
1 *UPLOAD OBJECT

E I FLE 1 FILE I FLE 3FLE 4FILE 3 FILESFLE SFILE 6
BON B1 B23 BO B1 B1 BO BIBI

I.

IR

L . . . - -- - - - -

FILE BLOCK
LOCATION TABLES
???' ??????

-

- - L- - - RESTORATION WORKER FILE !

FILE 1
BO BL B2

TABLES

.

.

. .
??? ??? ??? ??? ??? ? ? ?? ?? ?

????

?????

NODE 2

UPLOAD OBJECT

FILE I FILE FILE 2 FILE I FILE I FILE 2 FILE 1 FILE 1 FILE 2
B3 B1. BOBO BI | B | BO BIBI

atent Oct. 2, 2018 Sheet 1 of 11

US 10,089,187 B1

. 1 FIG
120
NODEN w 4NODEWORKEPROCES
:
BCOAMUKNIPLTES UPLOADEOBJECTS N3ODE N3ODEWORKEPROCES
BFLOCEKLOCATINTABLES
METADTABLES 2NODE NODEWORKEPROCES immense
CORDINATN1ODE PROCES N1ODEWORKEPROCES

atent Oct. 2, 2018 Sheet 2 of 11

US 10,089,187 B1

???????
UOPBLJEACTD LoETRUGuA BIOBI 1NODE F21ILE
w?????? - ---
??????

. F2IG
+Yh

* *

OUPBJLEOACDT west om womenwoon momwwal www www from

N2ODE

w w tAethts

w w * ** * The
F3ILE 1BO

tmheen

www yw ***** **mm mmm

WA wBO12F1ILE
B2

TFMEABOLDERTSAUIONBPLJDEOCAT TFMEABODLERTSAN2UOPBJLOEDACT
*

MW
WWWWWWWWWWWWW
F3ILE BO

F2ILE

?.????

WORKE PROCES BO

1NODE

·

I

B| ] B012O7

*

*

*

*

*

WORKEPROCES BOBI *
*
- -- - * - --

2NODE 3FLE

B2

1FILE

www

White

BO

W SINAPSDHOT Me

With

1URNIEPOQLUEDSAETD : , BR] ; ( )AN1OGE

,LF:12INLES

the

itot

W

thwihtteh

wou men www ww www
* * *
*

*

DSNAPSHOT EY

AR WE
*

?NR2UiEPQOLUEDSAETD

F;U3ILNE

:RBANOGE

W

*

WW

Mwtihteth

w when wwwwwwwwwwwww

-
TH WPF WWWYWW

*
CORDINATPROCES F w w w when
wwwwwwwwwwwwwwwwwwwwwwwwwwww

U . S . Patent Oct. 2, 2018 Sheet 3 of 11

US 10,089,187 B1

FILE SIZE

G1B5

3 . FIG

MOD TIME 1/Y1UKXYSEZX0BRZ MZ15/XYUSEZ0BXYR 1|ZYU/XSYZ0XER
USER D

GROUP 001 001 002

ITEM TYPE 1

wwwwwwwwwwwwww

wwwwwwwwwwwwwwwwww

PERMISON RWX -WX R R

XYZ TIME /XYZZYX / YZXYZX
SNAPHOT PL ID 001 001 002
TOOOOOOOO
FILE PATH 0F/010 /F0002 /FOO /0B5A0R
OOOOOOOOOOOOOOOOOOOOOOOOOOOOOO000000OOOOOOOOOOOOOOOOOOOOOOOOO
LIN 001 002 050

idido

outdoduction

putut

POLICY NAME POLICY POLICY POLICY

atent Oct. 2, 2018 Sheet 4 of 11

US 10,089,187B1

. F4IG

BB22
. 6 F1LE BB1I

BBOO

BO1I

1

2

RESTOAINWORKE 1UOPBJL5OECADT U4OPBtLJOEhACDT N1ODE F3I156|4ILE +KM SK THAT * *

1FILE

2NODE IF21|ILLE

-! wmoamn

f BBIOiB3LI

TuiBO * twmhatehat *
ww

WWW.

awin

* www

waawhaemewn

the

e

wwhen

LOCATINTABLES METADTABLES BI

it
whether

FBLIOLCEKww
Putheri
?

w L . -- - -
CAMWA

atent Oct. 2, 2018 Sheet 5 of 11

US 10,089,187B1

500m

MAINTAINING A DISTRIBUTED FILE SYSTEM OPERABLE WITHIN A CLUSTER OF
NODES WHEREIN FILES AND DIRECTORIES OF THE DISTRIBUTED FILE SYSTEM ARE
ASSOCIATED WITH A UNIQUE LOGICALINODE ("LIN ")AND A SET OF DATA BLOCKS
wwwwww

ESTABLISHING A CLOUD BACKUP POLICY WHEREIN THE CLOUD BACKUP POLICY IS

ASSOCIATED WITH A BACKUP SET OF FILES AND DIRECTORIES OF THE DISTRIBUTED

FILE SYSTEM

520

INITIATING A CLOUD BACKUP COORDINATOR PROCESS ON A COORDINATOR NODE

AMONG THE CLUSTER OF NODES

530

menanam

a

MARA MARA

LANA

Anne

INITIATING A SET OF WORKER PROCESSES BY THE CLOUDBACKUP COORDINATOR

PROCESS WHEREIN A WORKER PROCESS AMONG THE SET OF WORKER PROCESSES IS

ASSIGNED TO EACH NODE IN THE CLUSTER OF NODES , WHEREIN THE SETOF

WORKER PROCESSES ARE IN COMMUNICATION WITH THE CLOUD BACKUP

COORDINATOR PROCESS

540

WWWWWWWWWWWWWWWWWWWWWWW * WWWW

ASSIGNING AT LEAST ONE OF THE BACKUP SET OF FILES AND DIRECTORIES TO EACH

WORKER PROCESSES IN THE SET OF WORKER PROCESSES

550

FOR EACH WORKER PROCESS IN THE SET OF WORKER PROCESSES

PACKAGING THE AT LEAST ONE ASSIGNED BACKUP SET OF FILES AND

DIRECTORIES INTO A NODE LOCAL UPLOAD OBJECT WHEREIN THE PACKAGING

INCLUDES ARRANGING THE SET OF DATA BLOCKS ASSOCIATED WITH THE AT

LEAST ONE BACKUP SET OF FILES AND DIRECTORIES INTO THE NODE LOCAL

UPLOAD OBJECT AND GENERATING A SETOFMETADATA TABLES ASSOCIATED

WITH THE NODE LOCAL UPLOAD OBJECT

H

o ttes

t ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttth LALA

All that

there that with

the

561
that the winter

LAVAVAVAVAAN

NAMAMA

VAAVAAVAAVAA veevee

IN RESPONSE TO AT LEASTONE OF THE NODE LOCAL UPLOAD OBJECT REACHING AN
OBJECT CAPACITY OR THE WORKER PROCESS FINISHING PACKING THE AT LEAST ONE BACKUP SET OF FILES AND DIRECTORES INTO THE NODE LOCAL UPLOAD OBJECT, UPLOADING THE NODE LOCAL OBJECT AND THE SET OF METADATA TABLES ASSOCIATED WITH THE NODE LOCAL OBJECT TO A CLOUD STORAGE PROVIDER 370
Mentretien

COALESCING THE SET OF METADATA TABLES ASSOCIATED WITH EACH UPLOADED NODE LOCAL OBJECT INTO A CLOUD HOSTED METADATA TABLE AND A CLOUD HOSTED FILE BLOCK LOCATION TABLE
FIG . 5

atent Oct. 2, 2018 Sheet 6 of 11

US 10,089,187 B1

600
IN RESPONSE TO DETECTING AT LEAST ONE CHANGED FILE AND DIRECTORY IN THE BACKUP SET OF FILES AND DIRECTORIES , ASSIGNING BY THE COORDINATOR PROCESS THE ATLEAST ONE CHANGED FILE AND DIRECTORY TO WORKER
PROCESSES IN THE SET OF WORKER PROCESSES

PACKAGING THE AT LEAST ONE CHANGED FILE AND DIRECTORY INTO A CHANGED

NODE LOCAL UPDATE OBJECT WHEREIN THE PACKAGING INCLUDES ARRANGING A

SET OF CHANGED DATA BLOCKS ASSOCIATED WITH THE AT LEAST ONE CHANGED

FILE AND DIRECTORY AND GENERATING A SET OF CHANGED METADATA TABLES

ASSOCIATED WITH THE CHANGED NODE LOCAL UPLOAD OBJECT

620

COALESCING THE SET OF CHANGED METADATA TABLES WITH THE CLOUD HOSTED METADATA TABLE AND THE CLOUD HOSTED FILE BLOCK LOCATION TABLE

FIG . 6

atent Oct. 2, 2018 Sheet 7 of 11

US 10,089,187 B1

700.

DETERMINING AN UPLOADED NODE LOCAL UPLOAD OBJECT CONTAINS NO ACTIVE

REFERENCE WITHIN THE CLOUD HOSTED METADATA TABLE AND THE CLOUD

HOSTED FILE BLOCK LOCATION TABLE

710

IN RESPONSE TO THE DETERMINING , DELETING THE UPLOADED NODE LOCAL

UPLOAD OBJECT FROM THE CLOUD STORAGE PROVIDER

720

FIG . 7

atent Oct. 2, 2018 Sheet 8 of 11

US 10,089,187 B1

800

DETERMINING AN UPLOADED NODE LOCAL UPLOAD OBJECTHAS BEEN MOVED TO

NAARSAANSTAAS NA SMAASSAARE

COLD STORAGE

810

M AAARRRAAANNNAAM KARARAANANANANANANARAMANMAAR NAARSKUWA AZMA

R

NAMA RAMANANSANAAAAAAAAAAAANAASSAMESANANAMANSARA NARANMARAMAN KARSMANNAMAAN

w REQUESTING THE CLOUD STORAGE PROVDER RESTORE THE UPLOADED NODE LOCAL UPLOAD OBJECT FROM COLD STORAGE

wek IN RESPONSE TO THE UPLOADED NODE LOCAL UPLOADOBJECTBEING RESTORED FROM COLD STORAGE , RESTORING A SET OF FILES ASSOCIATED WITH THE UPLOADED NODE LOCAL UPLOAD OBJECT,WHEREIN THE SETOFFLES ASSOCIATED
WITH THE UPLOADED NODE LOCAL UPLOAD OBJECT ARE DETERMINED BASED ON
THE CLOUD HOSTED METADATA TABLE AND THE CLOUD HOSTED FILE BLOCK
LOCATION TABLE

FIG . 8

U. S . Patent 0 ct. 2,2018 Sheet 9 of 11

US 10,089,187 B1

POWER SUPPLY
???????????????????????
} } ??????­?????
36{
AA???????????????????????????????????
RACK LOGIC 9f88

?? 31.3

??
?

? 10° 4??}}?? % F2 ?? ???????????????????????????????????????????????? NODE
NODE 48? ? ? ?

CLUSTER OF NODES

}}}

-- - -- --- -- - - -- - -- -- - - -- - - - -- - -- - -- - -- -

- - --- -- --- -- - - - - -- - - - -- - - - - - -- -- -- - - - -- - - - - - -- - - - --

FIG. 9

U . S . Patent Oct. 2, 2018 Sheet 10 of 11 US 10,089,187 B1

PROCESSOR w1in0n0un2ni

MAR

w OPERATING SYSTEM 1012

BIOS 1014

w FILE STORAGE
1022
UPLOAD METADATA
1024
WWWWWWWWWWWWWWWWML
UPLOAD OBJECTS
1026

CLOUD BACKUP APPLICATIONS
1032
SNAPSHOT APPLCIATIONS
1034

POLICY DATA * 1028**

DATA STORAGE

APPLICATIONS

1020

1030

w

wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww
MEMORY 1010

NODE
1000
wenn

1/0 INTERFACE 1040
PER
PROCESSOR
READABLE STATIONARY
STORAGE 1050
PROCESSOR READABLE
REMOVABLE
STORAGE 1060
TRENILK

FIG . 10

atent Oct. 2, 2018 Sheet 11 of 11 US 10,089, 187 B1
.1FI1G
]S[O(NFBTIAJMZESRCET
B # B1OEIFLAOCSNF0KRHES
BASE BF,RINAODSEWRX-ABENLCRO9ODEKS
LIN OLIFNGIFLCDAE

US 10,089,187 B1

SCALABLE CLOUD BACKUP

specification, or any scope of the claims. Its sole purpose is

to present some concepts of the specification in a simplified

FIELD OF THE INVENTION

form as a prelude to the more detailed description that is

presented in this disclosure .

This invention relates generally to distributed file sys- 5 In accordance with an aspect, a cloud backup policy can

tems, and more particularly to implementations for a scal- be established wherein the cloud backup policy is associated

able cloud backup capability for a cluster of nodes operating with a backup set of files and directories of the distributed

as a distributed file system .

file system . A cloud backup coordinator process can be

BACKGROUND OF THE INVENTION

initiated on a coordinator node among the cluster of nodes.
A set of worker processes can be initiated by the cloud

Distributed file systems offer many compelling advan
tages in establishing high performance computing environ ments. One example is the ability to easily expand, even at
large scale. In one example, a distributed file system can operate under a cluster of nodes topology, whereby clients
can connect to any node among the cluster of nodes to perform file system activity . Individual nodes among the

15

backup coordinator process wherein a worker process among the set of worker processes is assigned to each node in the cluster of nodes, wherein the set ofworker processes pa4rreoceinss.coAmtmluenaistcaotnieoonftwhietbhactkhuepclsoeutdofbfailceksuapndcdoiorredcitnoartioers can be assigned to each worker processes in the set of worker processes. For each worker process in the set of

cluster of nodes each can contain their own processor(s), worker processes, at least one assigned backup set of files

storage drives, memory and the like. Operating together in 20 and directories can be packaged into a node local upload

a cluster, the nodes can respond to clientrequests, store data, object wherein the packaging includes arranging the set of

mirror data , and accomplish all the tasks of a modern file data blocks associated with the at least one backup set of

system . A cluster of nodes, in some cases, can provide easy files and directories into the node local upload object and

scalability by providing for new nodes to be added to the generating a set ofmetadata tables associated with thenode

cluster of nodes to increase the amount of storage space 25 local upload object. In response to at least one of the node

within the distributed file system and/or to meet other needs localupload object reaching an object capacity or theworker

of the users of the distributed file system .

process finishing, packaging the at least one backup setof

Some distributed file systemscan also regularly sync with
a backup cluster of nodes. The backup cluster ofnodes can operate as an independent file system to the primary cluster
of nodes . Data can be cloned on the backup cluster ofnodes
and periodically updated to include changes made on the primary cluster of nodes. For example, using a snapshot
based change identification model to changed files of the file
system , block based updates can be made to the backup
cluster of nodes that solely require sending modified por
tions of files to the backup cluster ofnodes. A backup cluster of nodes can operate to provide a safe backup from a potential disaster recovery where the primary cluster of

30
35

files and directories into the node local upload object, uploading the node local object and the set of metadata tables associated with the node local object to a cloud storage provider. The set of metadata tables associated with
each uploaded node local object can be coalesced into a
cloud hosted metadata table and a cloud hosted file block location table .
The following description and the drawings set forth certain illustrative aspects ofthe specification . These aspects
are indicative,however, of but a few of the various ways in which the principles of the specification may be employed .

nodes has suffered total failure. However, for some use 40 Other advantages and novel features of the specification will

cases, it may not be economical or feasible to locate a become apparent from the detailed description of the speci

backup cluster of nodes geographically separate from the fication when considered in conjunction with the drawings.

primary cluster of nodes such that a potential disaster does
not affect the backup cluster of nodes as well.
One common approach to disaster recovery is to use 45

BRIEF DESCRIPTION OF THE DRAWINGS

offsite Tape backup. Using a protocol such as NDMP, a file FIG . 1 illustrates an example cluster of nodes including a

system can dump an image of its data onto a physical tape coordinator process and worker processes facilitating scal

backup and that tape backup can then be geographically able cloud backup in accordance with implementations of

dispersed from the file system . However, tape backup can this disclosure;

consumeresources and timemaking it difficult to scale. One 50 FIG . 2 illustrates an example coordinator process, worker

means for addressing this is to use cloud storage services as processes, and objectpackaging in accordance with imple

the backup target. It can be appreciated that cloud storage mentations of this disclosure;

systems can be operated independent of the primary cluster FIG . 3 illustrates an examplemetadata table in accordance

of nodes and thereby less prone to a single a disaster event with implementations of this disclosure

affecting both the primary cluster of nodes and the cloud 55 FIG . 4 illustrates an example restoration worker in accor
storage backup . It can also be appreciated that cloud storage dance with implementations of this disclosure;

services can bemore efficient and more convenientthan tape FIG . 5 illustrates an example method for initiating a

backups.

coordinator and a set of workers to begin a scalable cloud

backup in accordance with implementations of this disclo

SUMMARY

60 sure;

FIG . 6 illustrates an example method for incrementally

The following presents a simplified summary of the synching the source cluster of nodes with the cloud backup

specification in order to provide a basic understanding of location in accordance with implementations of this disclo

some aspects of the specification . This summary is not an sure;

extensive overview of the specification . It is intended to 65 FIG . 7 illustrates an example method for cloud object

neither identify key or critical elements of the specification garbage collection in accordance with implementations of

nor delineate the scope of any particular embodiments of the this disclosure;

US 10 ,089,187 B1

FIG . 8 illustrates an example method for restoration from node and/or multiple nodes among the cluster of nodes,

cold storage in accordance with implementations of this thereby preventing failures of a node or a storage drive from

disclosure ;

disrupting access to data by the clients . Metadata , such as

FIG . 9 illustrates an example block diagram of rack of a inodes, for an entire distributed file system can bemirrored

cluster ofnodes in accordance with implementations of this 5 and/or synched across all nodes of the cluster ofnodes.

disclosure ;

The term "coordinator process" refers to a process that

FIG . 10 illustrates an example block diagram of a node in managesthe accomplishing oftasks acrossmultiple nodes of

accordance with implementations of this disclosure; and the cluster. The process can be established on a coordinator

FIG . 11 illustrates an example metadata table in accor node and can distribute a plurality of tasks to nodes among

dance with implementations of this disclosure .

10 the cluster of nodes. In some implementations, the coordi

nator process can assign tasks to the same node the coordi

DETAILED DESCRIPTION

nator process is executing on. In some implementations, the

coordinator process can spin up and down on differing nodes

The innovation is now described with reference to the of the cluster of nodes during the pendency of completing

drawings, wherein like reference numerals are used to refer 15 the set ofwork the process was designed to accomplish . For

to like elements throughout. In the following description, for example, a coordinator process can begin on a firstnode and

purposes of explanation , numerous specific details are set after assigning work itemsto a set ofworker processes, can

forth in order to provide a thorough understanding of this be spun down and later spun up a second node to further

innovation. It may be evident, however, that the innovation accomplish the coordinator process. In one implementation,

can be practiced without these specific details . In other 20 the coordinator process can communicate bi-directionally

instances, well-known structures and devices are shown in with worker processes to monitor the progress of worker

block diagram form in order to facilitate describing the processes.

innovation .

The term " worker process" refers to a node local process

The term " inode" or " logical inode" or "LIN " as used that receives requests and/or work items from the coordi

herein refers to data structures thatmay store information , or 25 nator process . Worker processes generally do not commu

meta-data , about files and folders, such as size, file owner- nicate with otherworker processes and are solely directed by

ship , access mode (read, write , execute permissions), time the coordinator process . Worker processes can communicate

and date of creation and modification , file type, or the like the status of their work items to the coordinator process.

In at least one of the various embodiments, inode data Implementations are provided herein for scalable cloud

structures may contain one or more references or pointer to 30 backup of a distributed file system . A coordinator process

the actual data blocks of the contents stored in the file . In at can be established thatuses worker processes on each node,

least one of the various embodiments, inodes may be in a or a subset of nodes, to package file system data that is

known location in a file system . From an inode, a reader or targeted for cloud backup into node local upload objects.

writer may access the contents of the inode and the contents File data can be arranged into distinct block offsets of the

of the file . Some file systems implement inodes using a data 35 node local upload object, and a single node local upload

structure called an inode. In at least one of the various object can contain data associated with multiple files and /or

embodiments, a data structure explicitly named " inode" may directories. A set of metadata tables can be generated that

be absent, but file systems may have data structures that characterize each file that is backed up as well as file block

store data similar to inodes and may provide capabilities location information for each data block of files that are

similar to inodes as described herein .

40 packaged within node local upload objects . The node local

As used herein , the term "node" refers to a physical upload objects can be packaged over time and once an object

computing device, including, but not limited to , network capacity is reached, or alternatively at other configurable

devices, servers, processors, cloud architectures, or the like. thresholds, the node local upload object can be uploaded to

A nodemay also include virtualmachines, or the like. In at a cloud service provider for storage on the cloud service

least one of the various embodiments, nodes may be 45 provider's storage platform . The set of metadata tables

arranged in a cluster interconnected by a high-bandwidth , generated by the worker process on the node when packag

low latency network backplane. In atleast one ofthe various ing the node local upload object can also be uploaded to the

embodiments, non -resident clients may communicate to the cloud service provider 's storage platform and coalesced into

nodes in a cluster through high -latency, relatively low - a global setofmetadata tables that describe the data thathas

bandwidth front side network connections, such as Ethernet, 50 been backed up, including block location data for uploaded

or the like .

objects .

The term " cluster of nodes" refers to one or more nodes In one implementation , after an initial cloud backup has

that operate together to form a distributed file system . In one occurred , a snapshot service of the file system can be used

example , a cluster of nodes forms a unified namespace for to incrementally backup blocks of files that have been

a distributed file system . Nodes within a cluster may com - 55 changed since the previous backup. It can be appreciated

municate information about nodes within the cluster to other that by using snapshots for backing up incremental changes

nodes in the cluster. Nodes among the cluster of nodes to the file system , network traffic can be minimized as

function using the same logical inode " LIN " mappings that individualblocks of files and/or directories unchanged since

describe the physical location of the data stored within the the previous snapshot should not need to be recopied to

file system . Clients can connect to any one node among the 60 cloud storage. It can be appreciated that as some files are

cluster ofnodes and access data stored within the cluster.For overwritten and/or modified, only a portion of the file is

example, if a client is connected to a node, and that client actually changed and some blocks of file data remain requests data that is not stored locally within the node, the unchanged . An incremental snapshot system can identify

node can then load the requested data from other nodes of changed blocks within a file that need to be backed up and

the cluster in order to fulfill the request of the client. Data 65 avoid backing up data blocks that already exist in the backup

protection plans can exist that stores copies or instances of location . For example , using a copy on write snapshot

file system data striped across multiple drives in a single system , as data is written to the source cluster, the newly

US 10,089,187 B1

written data blocks can be synced to the cloud storage nation, can be initiated on any node among the cluster of provider incrementally upon each write ,a batch ofwrites, or nodes. The coordinator process manages the overall set of

upon custom sync or snapshot settings . To support a batch of tasks necessary for scalable cloud backup . It can be appre

writes, changelists can bemaintained on the distributed file ciated that the coordinator process can be spun down after

system that aggregate changes to the file system since the 5 delegating work items to worker processes and then spun

last iterative cloud backup. For example, a snapshot can be back up when necessary to direct further actions. It can be

taken of the files targeted for cloud backup at the time the further appreciated thatthe coordinator process can be spun

data from the files is backed up , and the snapshot system can back up on a different node than Node 1 without compro

be used to track changes to snapshotted files.

mising the scalable cloud backup process .

In one implementation , some of the cloud backup objects 10 The coordinator process can initiate node local worker

can be moved from general purpose cloud storage to cold processes to process the coordinator processes instructions.

cloud storage to reduce the ongoing costs ofcloud backup. Each node in the clusterofnodes can be in communication

Cloud objects that are moved to cold cloud storage can be with external networks (e .g., network locations external to

flagged for use by a restoration process as described below . the cluster of nodes ) such as the cloud service provider

In one implementation , the set of metadata tables stored 15 depicted in FIG . 1. Nodes can upload objects to the cloud

within the cloud storage provider can be analyzed (e .g., by service provider and coalesce data into the metadata tables

downloading the tables from the cloud storage provider to and file block location tables as described in implementa

the distributed file system ) to determine if there are any tions of this disclosure. previously uploaded backup objects that no longer contain A cloud backup policy can be established whereby the

activereferences to file data that isnecessary forbackup. For 20 cloud backup policy can describe the framework for the

example, as incremental changes to files are stored in a cloud backup. For example, the cloud backup policy could

scalable cloud backup, some backup objects stored within apply to the entire file system or a subset of the files,

the cloud may contain data blocks that are no longer relevant directories and other data structures that make up the dis

(e.g., expired data ). It can be appreciated that in most object tributed file system . The cloud backup policy can also

based cloud storage systems, objects are notmodifiable, and 25 contain the name, network configuration, and authentication

therefore any incremental changes to files on the primary file information to the cloud services provider; a maximum

system are uploaded into new objects to cloud storage. cloud object size; a time limit for uploading incomplete

Therefore , as data ismodified , references to unmodified data (e.g ., less than maximum cloud object size) node local

within cloud objects may no longer be necessary to retain . upload objects ; snapshot identifiers associated with the

However, in some implementations, itmay not be desirable 30 cloud backup policy ; permissions for users who can modify

or possible to delete portions of a cloud object and that the the cloud policy, etc . It can be appreciated thatmultiple

entirety ofthe cloud objectmust expire before the object in cloud backup policies can exist for the distributed file

its entirety can be discarded .

system . For example , one cloud backup policy could backup

In one implementation , files can be restored from cloud a differentset of data than a second cloud backup policy. In
based backup . A worker can be established for each file / 35 another example , two cloud backup policies could reference

directory that needs to be restored . The worker can use the the same set of data , but one could backup data to a first

cloud hosted metadata table data and the cloud hosted file cloud services provider, and a second could backup data to

block location table data to determine the offset locations a second cloud services provider. within the cloud objects where the data blocks of the file to Referring now to FIG . 2 , there is illustrated an example

be restored are stored . The worker can then stitch the blocks 40 coordinator process, worker processes, and object packaging

together from different objects and/or the same object to in accordance with implementations of this disclosure. The

generate the file on the distributed file system . For example , coordinator process can send upload requests to worker

the restoration process can sequentially download from the processes on the various nodes of the cluster of nodes. The

cloud service provider only the block locations from objects coordinator process can manage the workflow of how many

the worker has assessed it needs to construct the file. 45 upload requests are sent to each node, and in some imple

In implementations where someobjects have been moved mentations,whether a node is sent any upload requests. For

to cold cloud storage , the restoration process can identify example , somenodesmay have fewer resources to devote to

objects that were flagged as such and initiate a restoration scalable cloud backup than other nodes due to existing

process thatmoves any objects necessary for the restoration workloads or non-uniform node hardware (e.g.,more or less

from cold cloud storage to normal cloud storage . It can be 50 CPU resources, memory resources, network resources, etc .).

appreciated that in some cloud storage environments, to In this example, a worker process on Node 1 is estab

support the reduced costs of cold cloud storage , the cloud lished , and the coordinator process has assigned it two

storage provider imposes a delay when accessing the data . upload requests. The upload requests can be made through

During the pendency of thedelay, therestoration process can the use of LINs of the files, as each file has a unique LIN

proceed with restoring data that is in normal cloud storage 55 associated with it. A snapshot identifier can also be associ

(e .g ., cloud storage locations with objects that are available ated with the request to ensure data integrity . For example ,

as immediate restoration targets ).

its possible data has been changed after the coordinator

Referring now to FIG . 1 , there is illustrated an example process assigned an upload , so through confirming a snap

cluster ofnodes including a coordinator process and worker shot identifier, the proper version of the file can be flagged

processes facilitating scalable cloud backup in accordance 60 for upload. Additionally, a block range of each file can be

with implementations of this disclosure. Node 1, Node 2, specified . For example, as discussed above with regard to
Node 3, and Node N (where " N " is an integer greater than incremental changes in a snapshot based system , only

3") represent a cluster of nodes thatmake up a distributed changed blocks after a file modification will need to be
file system . The nodes can run a common operating system , uploaded to cloud backup as unchanged blocks are already

such as EMC Isilon OneFS, and can be in communication 65 stored in existing cloud objects.

with each otherthrough a communicationsbackplane 120 . A In one implementation, using the LIN of the file, the

coordinator process , depicted on Node 1 for ease of expla - worker process can find a local block address location of the

US 10,089, 187 B1

distributed file system to find the data the coordinator object (including the size of each block ), a restoration

process is requestingbe uploaded into the cloud storage . The process could stitch together the complete file block by

worker process can continue to package individual data block .

blocks into an upload object. For example, Worker Process Referring now to FIG . 4, there is illustrated an example

Node 1, in response to the Node 1 upload requests, packages 5 restoration worker in accordance with implementations of

File 1-Block 0; File 1-Block 1 ; and File 2 -Block 1 into the this disclosure. In accordance with a restoration process,

Node 1 upload object. Worker Process Node 1 can continue
to wait for upload requests form the coordinator process to
keep filling Node 1 Upload Object. In one implementation , when Node 1 Upload object reaches a maximum capacity in size, the object can be uploaded to the cloud storage pro
vider. In one implementation, when Node 1 Upload object reaches a time limit of inactivity, a less than capacity sized
Node 1 Upload Object and be uploaded to cloud storage . In
one implementation, the Coordinator Process can instruct Worker Process Node 1 to package a less than capacity sized

10
15

objects that are needed for restoring are single file or a set
of files are identified. As discussed above, if some of the
objects necessary for restoration have been moved to cold storage, retrieval of the object from cold storage can be
initiated and the restoration can process can continue with
restoring files that are not dependent on objects then cur
rently stored within cold storage . Restoration workers can be
initiated for restoring a single backup item (e.g., a file). In FIG . 4 , Restoration Worker File 1 can look to the metadata

Node 1 Upload Object and upload it to cloud storage.

tables and the file block location tables to determine the

In one implementation , when Node 1 Upload Object is objects needed to restore the file and the block locations

uploaded to cloud storage, a new Node 1 Upload Object can within those objects that contain theblocksof the file. In this

be generated for new Node 1 Upload Requests received 20 example, Node 1 1st Upload Object contains Block 0 and

form the Coordinator Process.

Block 2 of the restoration file target (e.g., File 1) and Node

Worker Process Node 1 can also build a set of metadata 2 4th Upload Object contains Block 1 of the restoration

tables for the Node 1 Upload Object it is packaging. The target. The Restoration Worker File 1 can then stitch

contents of the metadata table it is packaging are described together the blocks from multiple different offsets from a

more fully with respect to FIG . 3 below . When uploading 25 single upload object and blocks from multiple objects into a

Node 1 Upload Object, the set of metadata tables for the restored file.

object can also be uploaded and coalesced with exiting FIGS. 5-8 illustrate methods and/or flow diagrams in

metadata tables already residing within cloud storage.

accordance with this disclosure . For simplicity of explana

In one implementation, in parallel to Worker Process tion, themethod is depicted and described as a series of acts.

Node 1 packaging blocks into Node 1 Upload Object and 30 However, acts in accordance with this disclosure can occur

constructing theMetadata Table for Node 1 Upload Object, in various orders and/or concurrently, and with other acts not

Worker Process Node 2 can be constructing Node 2 Upload presented and described herein . Furthermore , not all illus

Object and constructing the Metadata Table for Node 2 trated acts may be required to implement the methods in

Upload Object.

accordance with the disclosed subject matter. In addition ,

Referring now to FIG . 3 , there are illustrated an example 35 those skilled in the art will understand and appreciate that

metadata table in accordance with implementations of this the methods could alternatively be represented as a series of

disclosure. FIG . 3 illustrates an example cloud hosted meta - interrelated states via a state diagram or events.Additionally,

data table . The cloud hosted metadata table is maintained by it should be appreciated that the methods disclosed in this

coalescing new entries from worker processes on the cluster specification are capable of being stored on an article of

side. Example columns in the table can include a policy 40 manufacture to facilitate transporting and transferring such

name or identifier column that delineates which set of cloud methods to computing devices . The term article of manu

backup policies included the respective backup item . A LIN facture , as used herein , is intended to encompass a computer

column the lists the LIN of the item being backed up . The program accessible from any computer-readable device or

most recent snapshot identifier associated with the LIN . The storage media .

timestamp when this record was last updated. Permissions 45 Moreover, various acts have been described in detail

for the item (e.g., read,write, and execute permissions). The above in connection with respective system diagrams. It is

item type. In one implementation , the item type can be to be appreciated that the detailed description of such acts in

1 ­ Regular File; 2 -- ADS File; 3 -- Directory; 4 -- Deleted the prior figures can be and are intended to be implementable

File; and 5 -- Deleted Directory. Group ID and User ID in accordance with one or more of the following methods.

columns. A last modified timestamp for the item . A file/item 50 FIG . 5 illustrates an example method for initiating a

size. It can be appreciated that other columns not depicted coordinator and a set of workers to begin a scalable cloud

can also be recorded for backup items.

backup in accordance with implementations of this disclo

Referring now to FIG . 11, there is illustrated an example sure. At510, distributed file system operable within a cluster

metadata table in accordance with implementations of this ofnodes can bemaintained, wherein files and directories of

disclosure. FIG . 11 illustrates an example row from a cloud 55 the distributed file system are associated with a unique

hosted file block location table. A column associated with logical inode ("LIN ") and a set of data blocks. the LIN can identify the item . A base column can describe At 520 , a cloud backup policy can be established wherein

the base index for the row . For example, in one implemen - the cloud backup policy is associated with a backup set of

tation , each base index can be associated with a set of 10 files and directories of the distributed file system ;

data blocks in a base. It can be appreciated that in other 60 At 530, a cloud backup coordinator process can be

implementations a different amount of fixed blocks per initiated on a coordinator node among the cluster of nodes.

based index can be used , such as, for example, 100. The B # At 540, a set ofworker processes can be initiated by the

column can list for each of the 10 data blocks in the example cloud backup coordinator process wherein a worker process

base (e.g., BO-B9), a cloud hosted object namethat contains among the set of worker processes is assigned to each node

the block , a starting offset location where the block is 65 in the cluster of nodes, wherein the set of worker processes

located within the object, and a size of the block . It can be are in communication with the cloud backup coordinator

appreciated that by knowing the exact locations within an process.

US 10,089,187 B1

At 550, at least one of the backup set of files and include processors, power blocks, cooling apparatus, net

directories can be assigned to each worker processes in the work interfaces, input/output interfaces, etc. Although not

set of worker processes.

shown, cluster of nodes typically includes several computers

At 560, for each worker process in the set of worker thatmerely require a network connection and a power cord

processes, at 561 the at least one assigned backup set of files 5 connection to operate . Each node computer often includes

and directories can be packaged into a node local upload redundant components for power and interfaces. The cluster

object wherein the packaging includes arranging the set of of nodes 500 as depicted shows Nodes 910, 912, 914 and

data blocks associated with the at least one backup set of 916 operating in a cluster; however, it can be appreciated

files and directories into the node local upload object and thatmore or less nodes can make up a cluster. It can be

generating a set ofmetadata tables associated with the node 10 further appreciated that nodes among the cluster ofnodes do

local upload object.

not have to be in a same enclosure as shown for ease of

At570, in response to at least oneofthe node local upload explanation in FIG . 9, and can be geographically disparate.

object reaching an object capacity or the worker process Backplane 902 can be any type of commercially available

finishing , packaging the at least one backup set of files and networking infrastructure that allows nodes among the clus

directories into the node local upload object, uploading the 15 ter of nodes to communicate amongst each other in as close

node local object and the set ofmetadata tables associated to real timeas the networking infrastructure allows. It can be

with the node local object to a cloud storage provider. appreciated thatthe backplane 902 can also have a separate

At 580, the set of metadata tables associated with each power supply , logic, I/O , etc . as necessary to support com

uploaded node local object can be coalesced into a cloud munication amongst nodes of the cluster of nodes .

hosted metadata table and a cloud hosted file block location 20 As shown in the figure, enclosure 900 contains at least a

table. In one implementation, the cloud hosted file block power supply 904, an input/output interface 906 , rack logic

location table maps sets of a fixed number of blocks from 908, several nodes 910, 912, 914 , and 916 , and backplane

each backed up file to a set of cloud object locations.

902 . Power supply 904 provides power to each component

FIG . 6 illustrates an example method for incrementally and node within the enclosure. The input/output interface

synching the source cluster of nodes with the cloud backup 25 906 provides internal and external communication for com

location in accordance with implementations of this disclo - ponents and nodes within the enclosure . Backplane 908 can

sure. At 610, in response to detecting at least one changed enable passive and active communication of power, logic,

file and directory in the backup set of files and directories, input signals, and output signals for each node.

assigning by the coordinator process the at least one changed It can be appreciated that the Cluster of nodes 900 can be

file and directory to worker processes in the set of worker 30 in communication with a second Cluster of Nodes as

processes .

described in the subject disclosure and work in conjunction

At620 , the at least one changed file and directory can be to provide at least the implementations as disclosed herein .

packaged into a node local update object wherein the Nodes can refer to a physical enclosure with a varying

packaging includes arranging a set of changed data blocks amount of CPU cores, random access memory, flash drive

associated with the at least one changed file and directory 35 storage , magnetic drive storage , etc. For example, a single

and generating a set of changed metadata tables associated Node could contain , in one example, 46 disk drive bays with

with the changed node local upload object.

attached disk storage in each bay. It can be appreciated that

At 630 , the set of changed metadata tables can be nodes within the cluster of nodes can have varying configu

coalesced with the cloud hosted metadata table and the cloud rations and need not be uniform .

hosted file block location table .

40 FIG . 10 illustrates an example block diagram of a node

FIG . 7 illustrates an example method for cloud object 1000 in accordance with implementations of this disclosure.

garbage collection in accordance with implementations of As shown in FIG . 10 , a plurality of nodes may be included

this disclosure. At710, an uploaded node localupload object in one enclosure that shares resources provided by the

that contains no active references within the cloud hosted enclosure to reduce size, power, and cost.

metadata table and the cloud hosted file block location table 45 Node 1000 includes processor 1002 which communicates

can be determined . At 720, in response to the determination with memory 1010 via a bus. Node 1000 also includes

at 710 , the uploaded node local object form the cloud storage input/output interface 1040 , processor-readable stationary

provider can be deleted .

storage device(s) 1050 , and processor-readable removable

FIG . 8 illustrates an example method for restoration from storage device (s ) 1060. Input/output interface 1040 can

cold storage in accordance with implementations of this 50 enable node 1000 to communicate with other nodes,mobile

disclosure. At 810 , it can be determined whether an devices, network devices, and the like. Processor-readable

uploaded node local upload object has been moved to cold stationary storage device 1050 may include one or more

storage. At 820, a request can be sent to the cloud storage devices such as an electromagnetic storage device (hard

provider to restore the uploaded node local upload object disk ), solid state hard disk (SSD ),hybrid of both an SSD and

from cold storage. At 820, in response to the uploaded node 55 a hard disk , and the like . In someconfigurations, a node may

local upload objectbeing restored from cold storage, a setof include many storage devices. Also , processor-readable

files associated with the uploaded node local upload object removable storage device 1060 enables processor 1002 to

can be restored ,wherein the set of files associated with the read non -transitive storage media for storing and accessing

uploaded node local upload object can be determined based processor-readable instructions, modules, data structures,

on the cloud hosted metadata table and the cloud hosted file 60 and other forms of data . The non -transitive storage media

block location table.

may include Flash drives, tape media , floppy media, disc

FIG . 9 illustrates an example block diagram of a cluster media , and the like .

ofnodes in accordancewith implementations ofthis disclo Memory 1010 may include Random Access Memory

sure. However, the components shown are sufficient to (RAM ), Read -Only Memory (ROM ), hybrid of RAM and

disclose an illustrative implementation .Generally, a node is 65 ROM , and the like. As shown, memory 1010 includes

a computing device with a modular design optimized to operating system 1012 and basic input/output system

minimize the use of physical space and energy. A node can (BIOS) 1014 for enabling the operation of node 1000. In

US 10 ,089,187 B1

12

various embodiments, a general-purpose operating system
may be employed such as a version of UNIX , LINUXTM , FreeBSD , OneFS, a specialized server operating system such asMicrosoft's WindowsServerTM and Apple Computer's OS X , or the like.
Applications 1030 may include processor executable

cific implementations and examples are described herein for
illustrative purposes, variousmodifications are possible that are considered within the scope ofsuch implementations and examples, as those skilled in the relevant art can recognize.
In particular and in regard to the various functions per formed by the above described components, devices, cir

instructions which, when executed by node 1000 , transmit, cuits, systems and the like, the terms used to describe such
receive, and/or otherwise process messages, audio , video , components are intended to correspond, unless otherwise and enable communication with other networked computing indicated , to any component which performs the specified
devices. Examples of application programs include database 10 function of the described component ( e.g., a functional
servers, file servers, calendars, transcoders, and so forth . equivalent), even though not structurally equivalent to the
Applications 1030 may include, for example, cloud backup disclosed structure, which performs the function in the applications 1032 that support coordinator processes and herein illustrated exemplary aspects of the claimed subject

worker processes and snapshot applications 1034 according matter. In this regard , it will also be recognized that the

to implementations of this disclosure. It can be appreciated 15 innovation includes a system aswell as a computer-readable

that a UI for such application can exists, such as within a storage medium having computer-executable instructions

web UI for the distributed file system as a whole.

forperforming the acts and /or events of the variousmethods

Human interface components (not pictured ), may be of the claimed subject matter.

remotely associated with node 1000 , which can enable

remote input to and/oroutput from node 1000.For example, 20 What is claimed is:

information to a display or from a keyboard can be routed 1. A method comprising:

through the input/output interface 1040 to appropriate maintaining a distributed file system operable within a

peripheral human interface components that are remotely
located . Examples of peripheral human interface compo

cluster of nodes wherein files and directories of the
distributed file system are associated with a unique

nents include, but are not limited to , an audio interface, a 25 display, keypad , pointing device, touch interface, and the
like . Data storage 1020 may reside within memory 1010 as
well, storing file storage 1022 data such asmetadata or LIN data. It can beappreciated thatLIN data and /ormetadata can 30
relate to file storage within processor readable stationary

logical inode ("LIN ") and a set of data blocks; establishing a cloud backup policy wherein the cloud
backup policy is associated with a backup set of files
and directories of the distributed file system ; initiating a cloud backup coordinator process on a coor
dinator node among the cluster of nodes ;
initiating a set of worker processes by the cloud backup

storage 1050 and/or processor readable removable storage

coordinator process wherein a worker process among

1060. For example , LIN data may be cached in cache

the set of worker processes is assigned to each node in

memory for faster or more efficient frequent access versus

the cluster of nodes, wherein the set of worker pro

being stored within processor readable stationary storage 35 cesses are in communication with the cloud backup

1050 . In addition , Data storage 1020 can also store the

coordinator process ;

upload metadata 1024 and upload objects 1026 in accor assigning at least one of the backup set of files and

dance with implementations of this disclosure. Policy data

directories to each worker processes in the set of

1028 can be stored within data storage.

worker processes;

The illustrated aspects of the disclosure can be practiced 40 for each worker process in the set of worker processes:

in distributed computing environments where certain tasks

packaging at least one assigned backup set of files and

are performed by remote processing devices that are linked

directories into a node local upload object wherein

through a communications network . In a distributed com

the packaging includes arranging the set of data

puting environment, program modules can be located in

blocks associated with the at least one backup set of

both local and remote memory storage devices.

45

files and directories into the node local upload object

The systems and processes described above can be

and generating a set of metadata tables associated

embodied within hardware, such as a single integrated

with the node local upload object;

circuit (IC ) chip, multiple ICs, an application specific inte in response to at least one of the node localupload object

grated circuit (ASIC ), orthe like. Further, the order in which

reaching an object capacity or the worker process

some or all of the process blocks appear in each process 50 finishing packing the at least one backup set of files and

should notbe deemed limiting. Rather, it should be under

directories into the node local upload object, uploading

stood that some of the process blocks can be executed in a

the node local object and the set of metadata tables

variety of orders that are not all of which may be explicitly associated with the node local object to a cloud storage

illustrated herein .

provider; and

What has been described above includes examples of the 55 coalescing the set ofmetadata tables associated with each

implementations of the present disclosure. It is, of course,

uploaded node local object into a cloud hosted meta

not possible to describe every conceivable combination of

data table and a cloud hosted file block location table .

components or methods for purposes of describing the 2 . The method of claim 1 , wherein the cloud hosted

claimed subjectmatter, butmany further combinations and metadata table includes an entry for each backed up file and

permutations of the subject innovation are possible. Accord- 60 wherein each file entry includes at least a cloud backup

ingly, the claimed subjectmatter is intended to embrace all policy name, a LIN , a file path , a snapshot identifier, a

such alterations, modifications, and variations that fall timestamp, a set of file permissions, a file type, a group

within the spirit and scope of the appended claims.More- identifier, a user identifier, a last modification time, a parent

over, the above description of illustrated implementations of LIN , and a file size .

this disclosure, including what is described in the Abstract, 65 3. The method of claim 1, wherein the cloud hosted file is not intended to be exhaustive or to limit the disclosed block location table maps sets of a fixed number of blocks implementations to the precise formsdisclosed. While spe from each backed up file to a set of cloud object locations.

US 10,089, 187 B1

13

14

4. Themethod of claim 1, further comprising: In response to detecting at least one changed file and
directory in the backup set of files and directories,

the node local object and the set of metadata tables
associated with the node local object to a cloud storage
provider; and

assigning by the coordinator process the at least one coalescing the setofmetadata tables associated with each

changed file and directory to worker processes in the 5 uploaded node local object into a cloud hosted meta

set ofworker processes;

data table and a cloud hosted file block location table.

Packaging the at least one changed file and directory into a changed node local update object wherein the pack aging includes arranging a set of changed data blocks
associated with the at least one changed file and direc tory and generating a set of changed metadata tables
associated with the changed node local upload object;
and coalescing the set of changed metadata tables with the
cloud hosted metadata table and the cloud hosted file block location table. 5. The method of claim 4, further comprising:
determining an uploaded node local upload object con

10
15

8 . The system of claim 7 , wherein the cloud hosted
metadata table includes an entry for each backed up file and wherein each file entry includes at least a cloud backup
policy name, a LIN , a file path, a snapshot identifier, a
timestamp, a set of file permissions, a file type, a group
identifier, a user identifier, a last modification time, a parent
LIN , and a file size .
9. The system of claim 7, wherein the cloud hosted file block location table maps sets ofa fixed number ofblocks from each backed up file to a setof cloud object locations.
10. The system of claim 7, further configured to:

tains no active reference within the cloud hosted meta In response to detecting at least one changed file and

data table and the cloud hosted file block location table ; 20 directory in the backup set of files and directories ,

in response to the determining, delating the uploaded

assigning by the coordinator process the at least one

node local upload object from the cloud storage pro

changed file and directory to worker processes in the

vider.

set of worker processes;

6 . The method of claim 1, further comprising:

Packaging the at least one changed file and directory into

determining an uploaded node local upload object has 25 a changed node local update object wherein the pack

been moved to cold storage; requesting the cloud storage provider restore the uploaded
node local upload object from cold storage; in response to the uploaded node local upload object

aging includes arranging a set of changed data blocks
associated with the at least one changed file and direc
tory and generating a set of changed metadata tables
associated with the changed node local upload object;

being restored from cold storage, restoring a set offiles 30 and

associated with the uploaded node local upload object, coalescing the set of changed metadata tables with the

wherein the set of files associated with the uploaded

cloud hosted metadata table and the cloud hosted file

node local object are determined based on the cloud block location table .

hosted metadata table and the cloud hosted file block 11. The system of claim 10, further configured to :

location table .

35 determining an uploaded node local upload object con

7 . A system comprising at least one storage device and at least one hardware processor configured to :
maintaining a distributed file system operable within a

tains no active reference within the cloud hosted meta data table and the cloud hosted file block location table; in response to the determining, delating the uploaded

cluster of nodes wherein files and directories of the node local upload object from the cloud storage pro

distributed file system are associated with a unique 40 vider.

logical inode ("LIN " ) and a set of data blocks;

12 . The system of claim 7 , further configured to :

establishing a cloud backup policy wherein the cloud determining an uploaded node local upload object has

backup policy is associated with a backup set of files been moved to cold storage;

and directories of the distributed file system ;

requesting the cloud storage provider restore the uploaded

initiating a cloud backup coordinator process on a coor- 45 node local upload object from cold storage;

dinator node among the cluster of nodes ;

in response to the uploaded node local upload object

initiating a setofworker processes by the cloud backup being restored from cold storage, restoring a set of files

coordinator process wherein a worker process among

associated with the uploaded node local upload object,

the set ofworker processes is assigned to each node in

wherein the set of files associated with the uploaded

the cluster of nodes , wherein the set of worker pro - 50 node local object are determined based on the cloud

cesses are in communication with the cloud backup

hosted metadata table and the cloud hosted file block

coordinator process;

location table .

assigning at least one of the backup set of files and 13. A non-transitory computer readable medium with

directories to each worker processes in the set of program instructions stored thereon to cause a computer to

worker processes;

55 perform the following acts :

for each worker process in the set of worker processes : maintaining a distributed file system operable within a

packaging at least one assigned backup set of files and
directories into a node local upload object wherein the packaging includes arranging the set of data

cluster of nodes wherein files and directories of the
distributed file system are associated with a unique logical inode ("LIN " ) and a set of data blocks;

blocks associated with the at least one backup set of 60 establishing a cloud backup policy wherein the cloud

files and directories into the node local upload object backup policy is associated with a backup set of files

and generating a set of metadata tables associated and directories of the distributed file system ;

with the node local upload object;

initiating a cloud backup coordinator process on a coor

in response to at least one of the node local upload object

dinator node among the cluster ofnodes;

reaching an object capacity or the worker process 65 initiating a set of worker processes by the cloud backup

finishing packing the at least one backup set of files and
directories into the node localupload object,uploading

coordinator process wherein a worker process among the set ofworker processes is assigned to each node in

US 10,089,187 B1

15

the cluster of nodes, wherein the set ofworker pro In response to detecting at least one changed file and

cesses are in communication with the cloud backup

directory in the backup set of files and directories,

coordinator process; assigning at least one of the backup set of files and
directories to each worker processes in the set of 5 worker processes ; for each worker process in the set of worker processes :
packaging at least one assigned backup set of files and
directories into a node local upload object wherein

assigning by the coordinator process the at least one
changed file and directory to worker processes in the
set ofworker processes;
Packaging the at least one changed file and directory into a changed node local update object wherein the pack
aging includes arranging a set of changed data blocks
associated with the at least one changed file and direc tory and generating a set of changed metadata tables

the packaging includes arranging the set of data 10 associated with the changed node local upload object;

blocks associated with the at least one backup set of

and

files and directories into the node local upload object coalescing the set of changed metadata tables with the

and generating a set of metadata tables associated with the node local upload object; in response to at least one of the node local upload object
reaching an object capacity or the worker process
finishing packingthe at least onebackup set offiles and directories into thenode local upload object, uploading
the node local object and the set of metadata tables associated with the node local object to a cloud storage provider; and
coalescing the set ofmetadata tables associated with each

15 20

cloud hosted metadata table and the cloud hosted file
block location table . 17. The non -transitory computer readable medium of claim 16, with program instructions stored thereon to further
cause a computer to perform the following acts:
determining an uploaded node local upload object con
tains no active reference within the cloud hosted meta
data table and the cloud hosted file block location table ; in response to the determining, delating the uploaded
node local upload object from the cloud storage pro

uploaded node local object into a cloud hosted meta data table and a cloud hosted file block location table.
14. The non-transitory computer readable medium of
claim 13, wherein the cloud hosted metadata table includes an entry for each backed up file and wherein each file entry includes at least a cloud backup policy name, a LIN , a file
path, a snapshot identifier, a timestamp, a set offile permis-
sions, a file type, a group identifier, a user identifier, a last
modification time, a parent LIN , and a file size.
15 . The non -transitory computer readable medium of
claim 13, wherein the cloud hosted file block location table maps sets ofa fixed number ofblocks from each backed up file to a set of cloud object locations.
16 . The non -transitory computer readable medium of

2255
30
35

vider. 18. The non-transitory computer readable medium of claim 13 , with program instructions stored thereon to further ccaause a computer to perform the following acts: determining an uploaded node local upload object has
been moved to cold storage; requesting the cloud storage provider restore theuploaded
node local upload object from cold storage; in response to the uploaded node local upload object
being restored from cold storage, restoring a set of files
associated with the uploaded node localupload object, wherein the set of files associated with the uploaded
node local object are determined based on the cloud
hosted metadata table and the cloud hosted file block

claim 13, with program instructions stored thereon to further

location table .

cause a computer to perform the following acts:

????*

